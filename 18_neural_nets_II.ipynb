{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priya1016/NLP/blob/main/18_neural_nets_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "BgKMvFgB7C_g"
      },
      "source": [
        "# Lecture 18. Neural Networks II\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LqVm2ThS7Y5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture Activities \n",
        "\n",
        "Feel free to add cells to this notebook as you wish. Make sure to keep **code** and any **answers to questions** that you've written and turn in your notebook as a pdf at the end of the lecture. If you have not finished the exercises, don't worry, you can keep working on them until the end of the week.\n",
        "\n",
        "All lecture notebooks for a given week must be submitted to Gradescope by **11 pm on Sundays**.\n",
        "\n",
        "### Notebook submission\n",
        "\n",
        "Follow these steps to convert your notebook into a pdf for submission: \n",
        "1. Kernel -> Restart & Run All (clear your kernel's memory and run all cells)\n",
        "2. File -> Download As -> .html -> open in a browser -> print to pdf\n",
        "\n",
        "(The download as pdf option doesn't preserve formatting and output as nicely as taking the step \"through\" html, but will do if the above doesn't work for you.)"
      ],
      "metadata": {
        "id": "c0WpuD0_Gr0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Priya Garg"
      ],
      "metadata": {
        "id": "4CSIm-hfNX5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Task 1: Predict Word Sentiment\n",
        "\n",
        "We'll start by trying to train our neural net from last time to recognize if a word is positive or negative based on its word embedding.\n",
        "\n",
        "\n",
        "1. How many words were excluded from the train set because we didn't have vectors for them? What words were they?\n",
        "  \n",
        "  __433, mostly alphanumeric or containing special characters, all printed in task 1.1 cell__\n",
        "2. What happens to the error over time as the model runs?\n",
        "  \n",
        "  __error decreases suddenly first and then saturates__\n",
        "3. Based only on this, how well do you think that this model will perform? \n",
        "\n",
        "  __Since the error has saturated well, I expect the model to perform good__\n",
        "\n"
      ],
      "metadata": {
        "id": "wQoE3U9B8K8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wlZ52HkyY8En"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "%matplotlib inline\n",
        "# seed random number generation so that you can \n",
        "# track the same numbers as each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TAE8G8wgY8Eo"
      },
      "outputs": [],
      "source": [
        "# we'll get our embeddings from the gensim package\n",
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "izdy-pA_Y8Ep",
        "outputId": "8536a376-41bd-49f3-f7ac-8660d55821d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# go get some pre-trained word embeddings with 50 dimensions\n",
        "# glove refers to a different algorithm for creating the vectors\n",
        "# the resulting vectors follow the same structure:\n",
        "# dense vectors representing words in n-dimensional space\n",
        "\n",
        "# we're using this one because it's the smallest\n",
        "# set available pre-trained from gensim\n",
        "# 66 MB\n",
        "word2vec_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3cRec373Y8Eq",
        "outputId": "0bcefe3e-8496-4456-c6ec-20cc7a0f4509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.079084 -0.81504   1.7901    0.91653   0.10797  -0.55628  -0.84427\n",
            " -1.4951    0.13418   0.63627   0.35146   0.25813  -0.55029   0.51056\n",
            "  0.37409   0.12092  -1.6166    0.83653   0.14202  -0.52348   0.73453\n",
            "  0.12207  -0.49079   0.32533   0.45306  -1.585    -0.63848  -1.0053\n",
            "  0.10454  -0.42984   3.181    -0.62187   0.16819  -1.0139    0.064058\n",
            "  0.57844  -0.4556    0.73783   0.37203  -0.57722   0.66441   0.055129\n",
            "  0.037891  1.3275    0.30991   0.50697   1.2357    0.1274   -0.11434\n",
            "  0.20709 ]\n"
          ]
        }
      ],
      "source": [
        "# accessing a specific word vector\n",
        "print(word2vec_vectors['computer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CkJCbNRKY8Eq",
        "outputId": "2ca6eda1-fa27-4748-903d-4774597859f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num in train: 5410\n",
            "num in dev: 602\n"
          ]
        }
      ],
      "source": [
        "def load_word_list(filename: str, label: int) -> list:\n",
        "    \"\"\"\n",
        "    Loads a lexicon from a plain text file in the format of one word per line.\n",
        "    Parameters:\n",
        "    filename (str): path to file\n",
        "    label (int): label to assign the items in the file\n",
        "\n",
        "    Returns:\n",
        "    list: list of words\n",
        "    \"\"\"\n",
        "    # f = open(filename, \"r\", encoding=\"latin-1\")\n",
        "    # content = f.read()  # read all contents\n",
        "    # f.close()  # close the file when you're done\n",
        "    # lexicon = content.split('\\n')\n",
        "\n",
        "    # return lexicon\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        # skip the header content\n",
        "        # for line in f:\n",
        "        #     # print((line.strip(),label))\n",
        "        #     if line.strip() == \"\":\n",
        "        #         print(\"breaking\")\n",
        "        #         break\n",
        "        # read the rest of the lines into a list\n",
        "        return [(line.strip(), label) for line in f]\n",
        "        # f.close()\n",
        "    # otherwise return an empty list\n",
        "    return []\n",
        "\n",
        "def reduplicate(lst: list, times: int = 1000) -> list:\n",
        "    \"\"\"\n",
        "    Randomly reduplicates some number of elements of a list\n",
        "    and adds them to the list as it goes. Mutator function.\n",
        "    Parameters:\n",
        "    lst (list): list of items to reduplicate\n",
        "    times (int): number of times to reduplicate. Default to 1000.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    for i in range(times):\n",
        "        lst.append(lst[random.randrange(0, len(lst))])\n",
        "\n",
        "        \n",
        "# Load in word lists\n",
        "positives = load_word_list('data/positive_words.txt', 1)\n",
        "negatives = load_word_list('data/negative_words.txt', 0)\n",
        "\n",
        "# randomly re-duplicate some of the examples\n",
        "reduplicate(positives)\n",
        "reduplicate(negatives)\n",
        "\n",
        "# artifically balance the sets so we have the\n",
        "# same number of negatives and positives\n",
        "min_amt = min(len(positives), len(negatives))\n",
        "positives = positives[:min_amt]\n",
        "negatives = negatives[:min_amt]\n",
        "\n",
        "# TODO: Make a training set with 90% of the examples \n",
        "# and a dev set with 10% of the examples\n",
        "cut_off = int(min_amt*0.9)\n",
        "train = positives[:cut_off] + negatives[:cut_off]\n",
        "dev = positives[cut_off:] + negatives[cut_off:]\n",
        "\n",
        "\n",
        "print(\"num in train:\", len(train))\n",
        "print(\"num in dev:\", len(dev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "YVKN-G3oY8Er"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Apply the rectified linear function (max(0, x)) to each value in\n",
        "    an array of values.\n",
        "    Parameters:\n",
        "    x (array): array of values to apply function to\n",
        "\n",
        "    Returns:\n",
        "    numpy array of results\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_deriv(x):\n",
        "    \"\"\"\n",
        "    Apply the derivative of the rectified linear function to each value in\n",
        "    an array of values.\n",
        "    Parameters:\n",
        "    x (array): array of values to apply function to\n",
        "\n",
        "    Returns:\n",
        "    numpy array of results\n",
        "    \"\"\"\n",
        "    x[x<=0] = 0\n",
        "    x[x>1] = 1\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Apply the sigmoid function to each value in\n",
        "    an array of values.\n",
        "    Parameters:\n",
        "    x (array): array of values to apply function to\n",
        "\n",
        "    Returns:\n",
        "    numpy array of results\n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "    \"\"\"\n",
        "    Apply the derivative of the sigmoid function to each value in\n",
        "    an array of values.\n",
        "    Parameters:\n",
        "    x (array): array of values to apply function to\n",
        "\n",
        "    Returns:\n",
        "    numpy array of results\n",
        "    \"\"\"\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "azxtsgZ4Y8Es",
        "outputId": "75f88f1c-9763-412e-df0b-e0ed948c0a88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of inputs: (4977, 51)\n",
            "shape of labels: (4977, 1)\n",
            "number of excluded words:  433\n",
            "['a+', 'accessable', 'achievible', 'adorer', 'adulate', 'afordable', 'agilely', 'amiabily', 'awsome', 'beautifullly', 'believeable', 'benifits', 'better-than-expected', 'beutifully', 'brilliances', 'cashbacks', 'contrasty', 'convience', 'convienient', 'convient', 'courageousness', 'danken', 'dead-cheap', 'deginified', 'delightfulness', 'dirt-cheap', 'dotingly', 'dummy-proof', 'ecenomical', 'elate', 'elatedly', 'enrapt', 'enrapture', 'enthral', 'enviousness', 'ergonomical', 'err-free', 'exaltedly', 'exaltingly', 'examplar', 'examplary', 'excallent', 'exceled', 'excelent', 'excellant', 'excitedness', 'exellent', 'exhilarate', 'exultingly', 'eye-catch', 'eyecatch', 'fancinating', 'favorited', 'fecilitous', 'fervidly', 'fine-looking', 'first-in-class', 'futurestic', 'geekier', 'god-send', 'goood', 'gooood', 'hands-down', 'heroize', 'hotcake', 'imaculate', 'inestimably', 'ingenuously', 'inpressed', 'invaluablely', 'issue-free', 'jaw-droping', 'jollify', 'luckiness', 'marvelousness', 'merriness', 'miraculousness', 'pain-free', 'pamperedly', 'pamperedness', 'poeticize', 'preferes', 'problem-free', 'problem-solver', 'propitiously', 'prospros', 'raptureous', 'raptureously', 'recomend', 'rejoicingly', 'righten', 'rock-stars', 'shimmeringly', 'spellbind', 'spellbindingly', 'staunchness', 'stellarly', 'supurb', 'supurbly', 'thumb-up', 'titillatingly', 'trustingly', 'ultra-crisp', 'undisputably', 'upliftingly', 'user-replaceable', 'well-backlit', 'whooa', 'whoooa', 'wieldy', 'wonderous', 'wonderously', 'worth-while', 'ergonomical', 'wonderous', 'eye-catch', 'propitiously', 'upliftingly', 'dirt-cheap', 'ecenomical', 'poeticize', 'fervidly', 'pain-free', 'ergonomical', 'ecenomical', 'stellarly', 'adulate', 'excellant', 'issue-free', 'eye-catch', 'ergonomical', 'examplar', 'ecenomical', 'stellarly', 'problem-free', 'courageousness', 'delightfulness', 'ingenuously', 'accessable', 'problem-solver', 'beautifullly', 'exceled', 'staunchness', 'ecenomical', 'accessable', 'merriness', 'adulate', 'luckiness', 'raptureous', 'spellbindingly', 'shimmeringly', 'afordable', 'a+', '2-faced', '2-faces', 'abominate', 'absurdness', 'acerbate', 'achey', 'acridly', 'acridness', 'admonisher', 'admonishingly', 'adulterier', 'afflictive', 'aggrieve', 'aggrivation', 'angriness', 'anti-occupation', 'anti-proliferation', 'apathetically', 'appal', 'asininely', 'asinininity', 'asperse', 'assult', 'audaciousness', 'audiciously', 'avariciously', 'aweful', 'back-logged', 'back-wood', 'back-woods', 'backaching', 'backbite', 'backwood', 'banalize', 'bedlamite', 'befoul', 'beggarly', 'beleaguer', 'bereave', 'bid-rigging', 'bluring', 'bowdlerize', 'bragger', 'bulkyness', 'bull****', 'bull----', 'bullshyt', 'bullyingly', 'bumpping', 'burdensomely', 'calamitously', 'calumniate', 'calumniation', 'calumnious', 'calumniously', 'cataclysmal', 'cataclysmically', 'catastrophies', 'concen', 'concens', 'conscons', 'corrosions', 'corruptted', 'craftly', 'crowdedness', 'cruelness', 'cuplrit', 'd*mn', 'dastard', 'debaucher', 'defamations', 'defiler', 'degenerately', 'degradingly', 'deject', 'demolisher', 'demoralizingly', 'denunciate', 'deploringly', 'deprave', 'depravedly', 'derisiveness', 'desiccate', 'desititute', 'desolately', 'despoiler', 'despondence', 'destains', 'detestably', 'devilment', 'devilry', 'diappointed', 'dilly-dally', 'direness', 'dirts', 'disaffect', 'disaffirm', 'disagreeably', 'disapointed', 'disapointing', 'disapointment', 'discombobulate', 'discomfititure', 'discompose', 'disconsolation', 'discontentedly', 'discountenance', 'discourteously', 'discoutinous', 'disgruntle', 'disgustful', 'disgustfully', 'dishearteningly', 'dishonorablely', 'disillusions', 'dismalness', 'disoobedient', 'dispirit', 'dispiritedly', 'disquietude', 'disregardful', 'disrespectable', 'disrespectablity', 'disrespectfulness', 'dissappointed', 'dissappointing', 'dissatisfactory', 'dissatisfies', 'dissatisfy', 'dissembler', 'dissocial', 'dissonantly', 'distains', 'distastefully', 'distraughtly', 'distraughtness', 'disvalue', 'divisively', 'dizzing', 'dizzingly', 'dodgey', 'domineer', 'donside', 'douchbag', 'douchebag', 'douchebags', 'downfallen', 'downheartedly', 'dreadfulness', 'dubitable', 'dumbfound', 'egotistically', 'election-rigger', 'enervate', 'enfeeble', 'enviousness', 'exagerate', 'exagerated', 'exagerates', 'execrate', 'exorbitantance', 'expulse', 'f**k', 'fallaciously', 'fallaciousness', 'farcical-yet-provocative', 'fastuous', 'fat-cat', 'fat-cats', 'fatcat', 'fatcats', 'fatique', 'fatuity', 'fawningly', 'feeblely', 'flabbergast', 'flakieness', 'flicering', 'forebodingly', 'forgetfully', 'foully', 'fractiously', 'franticly', 'fustigate', 'gabble', 'gainsayer', 'gallingly', 'get-rich', 'gimmicked', 'gimmicking', 'gloatingly', 'god-awful', 'grumpiest', 'grumpish', 'hairloss', 'hard-liner', 'hasseling', 'hatefully', 'hatefulness', 'head-aches', 'heavyhearted', 'hestitant', 'hiliarious', 'ho-hum', 'hoodium', 'horrifys', 'hurted', 'hypocricy', 'ill-designed', 'ill-favored', 'ill-formed', 'ill-natured', 'ill-sorted', 'ill-usage', 'ill-used', 'immoderately', 'impenitent', 'importune', 'imposers', 'impossiblity', 'imprecate', 'inadverent', 'inadverently', 'inadvisably', 'inaptitude', 'incapably', 'incognizant', 'incompatability', 'incompliant', 'inconsequent', 'inconsequentially', 'inconsequently', 'inconsiderately', 'inconsistence', 'indecorum', 'indeterminably', 'indiscriminating', 'ineffectualness', 'inefficacious', 'inelegance', 'ineloquent', 'ineloquently', 'inexpertly', 'inexpiable', 'inexplainable', 'inhospitality', 'inimically', 'insociable', 'insubstantially', 'insupportably', 'insurmountably', 'intefere', 'inteferes', 'intolerablely', 'inveigle', 'invidiously', 'invidiousness', 'irately', 'irksomely', 'irksomeness', 'irksomenesses', 'irragularity', 'irrationalities', 'irrationals', 'irrecoverableness', 'irrecoverablenesses', 'irrecoverably', 'irreformable', 'irreplacible', 'irretating', 'jealousness', 'jeeringly', 'job-killing', 'judder', 'judders', 'jutter', 'jutters', 'laggy', 'layoff-happy', 'letch']\n"
          ]
        }
      ],
      "source": [
        "#Task 1.1\n",
        "\n",
        "# input should be in the format\n",
        "def create_input_array(examples: list, word_vectors: gensim.models.keyedvectors.Word2VecKeyedVectors) -> np.array:\n",
        "    \"\"\"\n",
        "    Convert examples to input and label arrays\n",
        "    Parameters:\n",
        "    examples (list): list of (word, label) tuples\n",
        "    word_vectors (gensim.models.keyedvectors.Word2VecKeyedVectors): dense word vectors to match with words\n",
        "\n",
        "    Returns:\n",
        "    numpy array of inputs, numpy array of corresponding labels\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    X = []\n",
        "    exclusions = []\n",
        "    for ex in examples:\n",
        "        word = ex[0]\n",
        "        label = ex[1]\n",
        "        # ignore words that we don't have vectors for\n",
        "        if word in word_vectors:\n",
        "            X.append(np.append(word_vectors[word], [1]))  # add in the bias term\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            exclusions.append(word)\n",
        "\n",
        "    return np.array(X, dtype=np.float128), np.array([labels]).T, exclusions\n",
        "\n",
        "X, y, exclusions = create_input_array(train, word2vec_vectors)\n",
        "print(\"shape of inputs:\", X.shape)\n",
        "print(\"shape of labels:\", y.shape)\n",
        "print(\"number of excluded words: \", len(exclusions))\n",
        "print(exclusions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Ipndh3U-Y8Et"
      },
      "outputs": [],
      "source": [
        "# hyper parameters\n",
        "# feel free to play with these!\n",
        "hidden_units = 20\n",
        "num_epochs = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "JwnDeRCNY8Et",
        "outputId": "73c00f57-f28b-4e07-c142-9b3d3bf56c7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: (51, 20)\n",
            "U: (20, 1)\n"
          ]
        }
      ],
      "source": [
        "# This is the same as the lecture 15 code\n",
        "\n",
        "input_features = X.shape[1]\n",
        "# initialize weights randomly with mean 0 and range [-1, 1]\n",
        "W_dim = (input_features, hidden_units)\n",
        "\n",
        "# you'll need to use W_dim and U_dim to produce the\n",
        "# correct number of random numbers\n",
        "W = np.array(2 * np.random.random(W_dim) - 1, dtype=np.float128)\n",
        "# note that we are doing binary classification, \n",
        "# so the second dimension here is 1 \n",
        "# (corresponding to one output unit)\n",
        "U_dim = (hidden_units, 1)\n",
        "U = np.array(2 * np.random.random(U_dim) - 1, dtype=np.float128)\n",
        "\n",
        "# make sure these numbers make sense\n",
        "print(\"W:\", W.shape)\n",
        "print(\"U:\", U.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "w8rnPgMgY8Eu",
        "outputId": "4dbd8726-2f15-4bbf-b9f6-6484214373fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Task 1.2 and 1.3\n",
        "\n",
        "inputs = X\n",
        "\n",
        "errors = []\n",
        "for i in range(num_epochs):\n",
        "    # forward propagation—sigmoid, relu, tanh, etc\n",
        "    h = relu(np.dot(inputs,W))\n",
        "    \n",
        "    # always sigmoid—classification\n",
        "    y_hat = sigmoid(np.dot(h,U))\n",
        "\n",
        "    # print(y.shape)\n",
        "    # print(y_hat.shape)\n",
        "    # print(type(y[0]))\n",
        "    # print(type(y_hat[0]))\n",
        "    # how much did we miss?\n",
        "    layer2_error = y - y_hat\n",
        "    \n",
        "    # what happens to our error as we go through epochs?\n",
        "    errors.append(sum(np.abs(layer2_error)))\n",
        "    \n",
        "    # this is telling us how much to move\n",
        "    # our weights and in what direction\n",
        "    # use the corresponding derivative to the non-linearity used above\n",
        "    layer2_delta = layer2_error * sigmoid_deriv(y_hat)\n",
        "\n",
        "    # how much did each L1 value contribute to \n",
        "    # the L2 error (according to the weights)?\n",
        "\n",
        "    layer1_error = layer2_delta.dot(U.T)\n",
        "    \n",
        "    # this is telling us how much to move\n",
        "    # our weights and in what direction\n",
        "    layer1_delta = layer1_error * relu_deriv(h)\n",
        "\n",
        "    U += h.T.dot(layer2_delta)\n",
        "    W += inputs.T.dot(layer1_delta)\n",
        "    \n",
        "plt.plot(range(num_epochs), errors)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "D-GGRJBnY8Ev"
      },
      "outputs": [],
      "source": [
        "def accuracy(y, y_hat):\n",
        "    \"\"\"\n",
        "    Measure the accuracy of our model, print the results.\n",
        "    Parameters:\n",
        "    y (array): true labels\n",
        "    y (array): model estimates\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    for i in range(len(y)):\n",
        "        guess = 1 if y_hat[i] > 0.5 else 0\n",
        "        if guess == y[i]:\n",
        "            count += 1\n",
        "    print(\"Accuracy:\", count / y.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6sLdVcacY8Ev",
        "outputId": "760f0bf8-77b2-47dd-ff48-00f415211a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output After Training:\n",
            "Accuracy: 0.5469026548672566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: overflow encountered in exp\n"
          ]
        }
      ],
      "source": [
        "print(\"Output After Training:\")\n",
        "# these are the same as the inputs that we trained this net on\n",
        "X_dev, y_dev,excl_test = create_input_array(dev, word2vec_vectors)\n",
        "\n",
        "# assign labels to the test data\n",
        "h = sigmoid(np.dot(X_dev,W))\n",
        "y_hat = sigmoid(np.dot(h,U))\n",
        "\n",
        "# These should match with each other\n",
        "accuracy(y_dev, y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Task 2: Predict Word Sentiment with Keras\n",
        "\n",
        "Now, we'll use some NN libraries that can handle numbers with lots of decimal points much better to see what happens with this task.\n",
        "\n",
        "Installation of component libraries (via `pip3`):\n",
        "\n",
        "```\n",
        "pip3 install tensorflow\n",
        "sudo pip3 install keras\n",
        "```\n",
        "\n",
        "\n",
        "1. How well did your model do? Theories about any differences that you saw with the two models?\n",
        "\n",
        "__The NN model (acc= ~89%) performed much better than previous model (acc= ~55%). Adding Dense layer is helpful as it captures the hidden context more.__"
      ],
      "metadata": {
        "id": "IQcLJ9WrqDE0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Bq7lTvKAY8Ex"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "0WHQs3z0Y8Ey",
        "outputId": "a5cd367b-8c58-489d-bd39-7446bdc7ab00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f286cc1f090>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# set up the basis for a feed forward network\n",
        "model = Sequential()\n",
        "# hidden layer -- same number of hidden units as above\n",
        "model.add(Dense(units=hidden_units, activation='relu', input_dim=X.shape[1]))\n",
        "# output layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# configure the learning process\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X, y, epochs=num_epochs, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "7O63Sx4nY8Ey",
        "outputId": "7f896469-4121-4a97-e67b-a63e372a59e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.8902654867256637\n"
          ]
        }
      ],
      "source": [
        "#Task 2.1\n",
        "\n",
        "y_hat = model.predict(X_dev)\n",
        "\n",
        "accuracy(y_dev, y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ijA249qLqBUO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}